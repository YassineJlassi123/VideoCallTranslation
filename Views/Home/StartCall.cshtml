@{
    ViewData["Title"] = "Video Call with Translation";
}

<h2 class="text-lg md:text-xl font-semibold text-center mb-4">Video Call with Translation</h2>

<div id="videoContainer" class="flex flex-col md:flex-row justify-center items-center mb-4">
    <video id="localVideo" autoplay muted class="w-full md:w-1/2 h-auto rounded-lg shadow-md"></video>
    <video id="remoteVideo" autoplay class="w-full md:w-1/2 h-auto rounded-lg shadow-md"></video>
</div>

<div id="translationContainer" class="max-w-lg mx-auto p-4 bg-white rounded-lg shadow-md">
    <div class="mb-4">
        <label for="sourceLang" class="block text-sm font-medium text-gray-700">Source Language:</label>
        <select id="sourceLang" class="mt-1 block w-full p-2 border border-gray-300 rounded-md shadow-sm focus:ring focus:ring-primary">
            <option value="en-US">English</option>
            <option value="es-ES">Spanish</option>
            <option value="fr-FR">French</option>
            <option value="ar-SA">Arabic</option>
            <option value="ar-TN">Tunisian Arabic</option>
            <option value="ja-JP">Japanese</option>
            <option value="ko-KR">Korean</option>
            <option value="ru-RU">Russian</option>
        </select>
    </div>
    <div class="mb-4">
        <label for="targetLang" class="block text-sm font-medium text-gray-700">Target Language:</label>
        <select id="targetLang" class="mt-1 block w-full p-2 border border-gray-300 rounded-md shadow-sm focus:ring focus:ring-primary">
            <option value="es-ES">Spanish</option>
            <option value="en-US">English</option>
            <option value="fr-FR">French</option>
            <option value="ar-SA">Arabic</option>
            <option value="ar-TN">Tunisian Arabic</option>
            <option value="ja-JP">Japanese</option>
            <option value="ko-KR">Korean</option>
            <option value="ru-RU">Russian</option>
        </select>
    </div>
    <div id="controlsContainer" class="mt-4">
        <button id="endCallBtn" class="w-full bg-red-500 text-white p-2 rounded-md hover:bg-red-600 transition-colors">End Call</button>
    </div>
    <div id="recognitionStatus" class="text-sm text-gray-600 mt-2"></div>
    <div id="translatedText" class="text-gray-600 mt-4"></div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/microsoft-signalr/8.0.0/signalr.min.js"></script>
<script src="https://webrtc.github.io/adapter/adapter-latest.js"></script>
<script src="https://code.responsivevoice.org/responsivevoice.js?key=Gdf5sSL4"></script>
<script>
    const roomId = '@ViewData["RoomId"]';
    let localStream, peerConnection, recognition, audioContext, mediaStreamSource, mediaStreamDestination;
    let isListening = false;
    let isSpeaking = false;
    let isRecognitionActive = false;
    let silenceTimer;
    const silenceThreshold = 1500; // 1.5 seconds of silence to stop listening
    const speechQueue = []; // Queue for handling recognized speech
    let isProcessingQueue = false; // Flag to check if queue is being processed

    const configuration = { iceServers: [{ urls: 'stun:stun.l.google.com:19302' }] };

    const connection = new signalR.HubConnectionBuilder()
        .withUrl("/VideoCallHub")
        .build();

    async function startCall() {
        try {
            // Request only video, no audio
            localStream = await navigator.mediaDevices.getUserMedia({ video: true, audio: false });
            const localVideo = document.getElementById('localVideo');
            if (localVideo) {
                localVideo.srcObject = localStream;
            } else {
                console.error("Local video element not found");
                return;
            }

            peerConnection = new RTCPeerConnection(configuration);

            // Only add video tracks, no audio tracks
            localStream.getVideoTracks().forEach(track => peerConnection.addTrack(track, localStream));

            peerConnection.ontrack = (event) => {
                const remoteVideo = document.getElementById('remoteVideo');
                if (remoteVideo) {
                    remoteVideo.srcObject = event.streams[0];
                } else {
                    console.error("Remote video element not found");
                }
            };

            peerConnection.onicecandidate = (event) => {
                if (event.candidate) {
                    connection.invoke("SendIceCandidate", roomId, JSON.stringify(event.candidate));
                }
            };

            await connection.start();
            console.log('SignalR connection established.');
            await connection.invoke("JoinRoom", roomId);

            const offer = await peerConnection.createOffer();
            await peerConnection.setLocalDescription(offer);
            await connection.invoke("SendOffer", roomId, JSON.stringify(offer));

        } catch (error) {
            console.error("Error starting call:", error);
        }
    }

    function startSpeechRecognition() {
        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
            console.error("Speech recognition not supported.");
            return;
        }

        if (isRecognitionActive) {
            console.log("Recognition is already active.");
            return;
        }

        recognition = new (window.webkitSpeechRecognition || window.SpeechRecognition)();

        recognition.continuous = true;
        recognition.interimResults = false;
        recognition.lang = document.getElementById('sourceLang').value;

        recognition.onstart = () => {
            isListening = true;
            isRecognitionActive = true;
            document.getElementById('recognitionStatus').textContent = 'Listening...';
        };

        recognition.onend = () => {
            isRecognitionActive = false;
            if (isListening && !isSpeaking) {
                setTimeout(() => {
                    if (!isRecognitionActive) {
                        recognition.start();
                    }
                }, 1000);
            }
        };

        recognition.onresult = handleRecognitionResult;

        recognition.onerror = (event) => {
            console.error("Speech recognition error", event.error);
            document.getElementById('recognitionStatus').textContent = `Error: ${event.error}`;
            isRecognitionActive = false;
            if (event.error === 'no-speech') {
                restartRecognition();
            }
        };

        recognition.start();
    }

    function handleRecognitionResult(event) {
        let finalTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; ++i) {
            if (event.results[i].isFinal) {
                finalTranscript += event.results[i][0].transcript;
            }
        }

        if (finalTranscript) {
            resetSilenceTimer();
            addToSpeechQueue(finalTranscript);
        }
    }

    function addToSpeechQueue(text) {
        speechQueue.push(text);
        if (!isProcessingQueue) {
            processSpeechQueue();
        }
    }

    async function processSpeechQueue() {
        if (speechQueue.length === 0 || isSpeaking) {
            isProcessingQueue = false;
            return;
        }

        isProcessingQueue = true;
        const sentence = speechQueue.shift();
        try {
            const translation = await translateText(sentence);
            document.getElementById('translatedText').textContent = translation;
            if (translation) {
                await speakTranslation(translation);
            }
        } catch (error) {
            console.error("Error in processing queue:", error);
        }

        // Process the next sentence in the queue after speaking finishes
        if (speechQueue.length > 0) {
            processSpeechQueue();
        } else {
            isProcessingQueue = false;
        }
    }

    async function translateText(text) {
        const sourceLang = document.getElementById('sourceLang').value;
        const targetLang = document.getElementById('targetLang').value;

        try {
            const response = await fetch('/Home/Command', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ Text: text, SourceLang: sourceLang, TargetLang: targetLang }),
            });

            if (!response.ok) throw new Error("Network response was not ok");

            const data = await response.json();
            return data.translatedText;
        } catch (error) {
            console.error("Translation error:", error);
            return `Translation failed: ${error.message}`;
        }
    }

    function speakTranslation(translation) {
        return new Promise((resolve) => {
            const targetLang = document.getElementById('targetLang').value;
            const voiceMap = {
                'en-US': 'US English Female',
                'es-ES': 'Spanish Female',
                'fr-FR': 'French Female',
                'ar-SA': 'Arabic Male',
                'ar-TN': 'Arabic Male',
                'ja-JP': 'Japanese Female',
                'ko-KR': 'Korean Female',
                'ru-RU': 'Russian Female'
            };

            const voice = voiceMap[targetLang] || 'UK English Female';

            // Stop recognition when speaking starts
            if (isRecognitionActive) {
                recognition.stop();
                isRecognitionActive = false;
            }

            isSpeaking = true;
            responsiveVoice.speak(translation, voice, {
                onstart: () => {
                    // Stop recognition immediately when speaking starts
                    if (recognition && isRecognitionActive) {
                        recognition.stop();
                    }
                },
                onend: () => {
                    isSpeaking = false;
                    // Resume recognition after speaking ends
                    setTimeout(() => {
                        if (!isSpeaking && isListening && !isRecognitionActive) {
                            recognition.start();
                            isRecognitionActive = true;
                        }
                    }, 1000);
                    resolve();
                }
            });
        });
    }

    function restartRecognition() {
        if (recognition) {
            if (isRecognitionActive) {
                recognition.stop();
            }
            setTimeout(() => {
                if (!isSpeaking && !isRecognitionActive) {
                    recognition.start();
                }
            }, 1000);
        }
    }

    function resetSilenceTimer() {
        clearTimeout(silenceTimer);
        silenceTimer = setTimeout(() => {
            console.log("Silence detected, restarting listening");
            restartRecognition();
        }, silenceThreshold);
    }

    function initializeApp() {
        startSpeechRecognition();
        const endCallBtn = document.getElementById('endCallBtn');
        if (endCallBtn) {
            endCallBtn.onclick = async () => {
                endCall();
                await connection.invoke("EndCall", roomId);
            };
        } else {
            console.error("End Call button not found");
        }

        const sourceLangSelect = document.getElementById('sourceLang');
        if (sourceLangSelect) {
            sourceLangSelect.onchange = () => {
                if (recognition) {
                    recognition.lang = sourceLangSelect.value;
                    restartRecognition();
                }
            };
        } else {
            console.error("Source language select not found");
        }

        startCall();
    }

    function endCall() {
        if (localStream) {
            localStream.getTracks().forEach(track => track.stop());
        }
        if (peerConnection) {
            peerConnection.close();
        }
        if (recognition) {
            recognition.stop();
        }
        isListening = false;
        isSpeaking = false;
        isRecognitionActive = false;
        document.getElementById('recognitionStatus').textContent = 'Call ended';
    }

    document.addEventListener('DOMContentLoaded', initializeApp);

    // SignalR connection handlers
    connection.on("ReceiveOffer", async (offer) => {
        await peerConnection.setRemoteDescription(new RTCSessionDescription(JSON.parse(offer)));
        const answer = await peerConnection.createAnswer();
        await peerConnection.setLocalDescription(answer);
        await connection.invoke("SendAnswer", roomId, JSON.stringify(answer));
    });

    connection.on("ReceiveAnswer", async (answer) => {
        await peerConnection.setRemoteDescription(new RTCSessionDescription(JSON.parse(answer)));
    });

    connection.on("ReceiveIceCandidate", async (candidate) => {
        await peerConnection.addIceCandidate(new RTCIceCandidate(JSON.parse(candidate)));
    });
</script>
